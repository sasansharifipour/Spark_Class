{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WORD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxZofkDiioG0ykKjqCVm1T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasansharifipour/Spark_Class/blob/main/WORD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB2SLfLo4pVO",
        "outputId": "ba1c83bf-f8a1-4d4e-cab8-8d8557151786"
      },
      "source": [
        "!pip install gdown\n",
        "!pip install parsivar\n",
        "\n",
        "!gdown https://drive.google.com/u/0/uc?id=1N3hrFBDdOKyFv43LjSe5ynTdu_E0wOfP\n",
        "!gdown https://drive.google.com/u/0/uc?id=1Cr3WJteka1SPQxi3-7swPobVmkwhwRp2\n",
        "!gdown https://drive.google.com/u/0/uc?id=1zUwtMBlAfxlP69T_-Ldmb9DW3WzdfnPa\n",
        "!gdown https://drive.google.com/u/0/uc?id=13gL9tfS3vwesBIcLtBnlSS3fTlZkWbDQ"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: parsivar in /usr/local/lib/python3.7/dist-packages (0.2.3)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from parsivar) (3.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->parsivar) (1.15.0)\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1N3hrFBDdOKyFv43LjSe5ynTdu_E0wOfP\n",
            "To: /content/train_set.csv\n",
            "46.9MB [00:00, 178MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1Cr3WJteka1SPQxi3-7swPobVmkwhwRp2\n",
            "To: /content/test_set.csv\n",
            "12.1MB [00:00, 105MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1zUwtMBlAfxlP69T_-Ldmb9DW3WzdfnPa\n",
            "To: /content/vectors.txt\n",
            "118MB [00:00, 153MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=13gL9tfS3vwesBIcLtBnlSS3fTlZkWbDQ\n",
            "To: /content/StopWords.csv\n",
            "100% 3.27k/3.27k [00:00<00:00, 5.69MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKvYCmqH7etT"
      },
      "source": [
        "import csv\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "import csv\n",
        "\n",
        "def prepare_output(text_Ids, y_pred):\n",
        "\n",
        "  header = ['app_id', 'label']\n",
        "  f = open('prediction.csv', 'w')\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(header)\n",
        "\n",
        "  i = 0\n",
        "  while i < len(text_Ids):\n",
        "    data = [text_Ids[i], y_pred[i]]\n",
        "    writer.writerow(data)\n",
        "    i += 1\n",
        "  \n",
        "  f.close()\n",
        "\n",
        "def remove_emoji(string):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"\n",
        "                           u\"\\U0001F680-\\U0001F6FF\" \n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
        "                           u\"\\U0001F917-\\U0001F9D0\"\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', string)\n",
        "\n",
        "def replace_arabic_persian_alphabet(str):\n",
        "  str = str.replace('ي','ی')\n",
        "  str = str.replace('ك','ک')\n",
        "  return str\n",
        "\n",
        "def remove_html_tags(str):\n",
        "  soup = BeautifulSoup(str, 'lxml')\n",
        "  return soup.get_text()\n",
        "\n",
        "def remove_half_space(str):\n",
        "  return str.replace('\\u200c', ' ')\n",
        "\n",
        "def remove_special_symbols(str):\n",
        "  str = str.replace(u'\\xa0', u' ')\n",
        "  str = str.replace(u'\\u200f', u' ')\n",
        "  str = str.replace(u'\\n', u' ')\n",
        "  str = str.replace(u'(', u' ')\n",
        "  str = str.replace(u')', u' ')\n",
        "  str = str.replace(u'.', u' ')\n",
        "  str = str.replace(u'-', u' ')\n",
        "  str = str.replace(u'،', u' ')\n",
        "  str = str.replace(u'=', u' ')\n",
        "  str = str.replace(u',', u' ')\n",
        "  str = str.replace(u'?', u' ')\n",
        "  str = str.replace(u'!', u' ')\n",
        "  str = str.replace(u'؟', u' ')\n",
        "  str = str.replace(u':', u' ')\n",
        "  return str\n",
        "\n",
        "def remove_white_space(str):\n",
        "  return re.sub(' +', ' ', str)\n",
        "\n",
        "def preprocess_text(str):\n",
        "  txt = replace_arabic_persian_alphabet(str.lower())\n",
        "  txt = remove_html_tags(txt)\n",
        "  txt = remove_half_space(txt)\n",
        "  txt = remove_special_symbols(txt)\n",
        "  txt = remove_emoji(txt)\n",
        "  txt = remove_white_space(txt)\n",
        "  #txt = remove_punctuation_char(txt)\n",
        "  return txt\n",
        "    \n",
        "def load_train_data():\n",
        "  with open('train_set.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    X = []\n",
        "    Y = []\n",
        "    for row in csv_reader:\n",
        "      if line_count == 0:\n",
        "        line_count += 1\n",
        "      else:\n",
        "        txt = preprocess_text(row[1])\n",
        "        X.append(txt)\n",
        "        Y.append(int(row[2]))\n",
        "        line_count += 1\n",
        "  return X, Y\n",
        "\n",
        "def load_test_data():\n",
        "  with open('test_set.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    X = []\n",
        "    X_IDs = []\n",
        "    for row in csv_reader:\n",
        "      if line_count == 0:\n",
        "        line_count += 1\n",
        "      else:\n",
        "        X_IDs.append(row[0])\n",
        "        X.append(preprocess_text(row[1]))\n",
        "        line_count += 1\n",
        "  return X,X_IDs\n",
        "\n",
        "def prepare_train_data(train_data, stop_words):\n",
        "  max_sentence_len = 8000\n",
        "  max_cnt = 0\n",
        "  train_len = len(train_data)\n",
        "  train_x = np.zeros([train_len, max_sentence_len], dtype=np.int32)\n",
        "\n",
        "  for i in range(train_len):\n",
        "    train_x[i,:], cnt = get_sentence_embedding(train_X[i], stop_words, max_sentence_len)\n",
        "    if (cnt > max_cnt):\n",
        "      max_cnt = cnt\n",
        "  \n",
        "  train_x = train_x[:,:max_cnt]\n",
        "  return train_x, max_cnt\n",
        "\n",
        "def prepare_test_data(test_data, stop_words, max_sentence_len):\n",
        "  test_len = len(test_data)\n",
        "  test_x = np.zeros([test_len, max_sentence_len], dtype=np.int32)\n",
        "\n",
        "  for i in range(test_len):\n",
        "    test_x[i,:], cnt = get_sentence_embedding(test_X[i], stop_words, max_sentence_len)\n",
        "\n",
        "  return test_x\n",
        "\n",
        "def get_sentence_embedding(sentence, stopwords, max_sentence_len):\n",
        "  words = sentence.split()\n",
        "  sentence_embeddings = np.zeros(max_sentence_len)\n",
        "  cnt = 0\n",
        "  for word in words:\n",
        "    if (word not in stop_words):\n",
        "      word_info = word_model.wv.vocab.get(word)\n",
        "      if (word_info != None):\n",
        "        sentence_embeddings[cnt] = word_info.index\n",
        "        cnt += 1\n",
        "      if (cnt == max_sentence_len):\n",
        "        break\n",
        "  return sentence_embeddings, cnt\n",
        "\n",
        "def load_stop_words():\n",
        "  with open('StopWords.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    X = []\n",
        "    for row in csv_reader:\n",
        "        X.append(replace_arabic_persian_alphabet(row[0]))\n",
        "\n",
        "  return X\n",
        "\n",
        "glove_path = 'vectors.txt'\n",
        "glove_filename = 'vectors'\n",
        "\n",
        "word2vec_output_file = glove_filename+'.word2vec'\n",
        "glove2word2vec(glove_path, word2vec_output_file)\n",
        "word_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P7GG8BOGWxu"
      },
      "source": [
        "train_X, train_Y = load_train_data()\n",
        "test_X, test_IDs = load_test_data()\n",
        "stop_words = load_stop_words()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CZH4TqFD8pK",
        "outputId": "7e47482d-1121-4496-d197-46624dc7e50f"
      },
      "source": [
        "train_x, max_sentence_len = prepare_train_data(train_X, stop_words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRVJDtF_D-gK",
        "outputId": "88a57e49-0dca-40a1-8b0f-22dc8b502840"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcSag5j_rgzm",
        "outputId": "024a95af-29ff-4cc3-a394-a746a133dae3"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "\n",
        "pretrained_weights = word_model.wv.syn0\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "\n",
        "train_x, max_sentence_len = prepare_train_data(train_X, stop_words)\n",
        "test_x = prepare_test_data(test_X, stop_words, max_sentence_len)\n",
        "\n",
        "train_y = np_utils.to_categorical(train_Y)\n",
        "\n",
        "lstm_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.summary()\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "history = lstm_model.fit(x= train_x, y= train_y, epochs=NUM_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 50)          12027400  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, None, 128)         58880     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, None, 64)          41216     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 12,157,138\n",
            "Trainable params: 12,157,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1185/1185 [==============================] - 695s 579ms/step - loss: 1.2704 - accuracy: 0.5339\n",
            "Epoch 2/10\n",
            "1025/1185 [========================>.....] - ETA: 1:32 - loss: 0.9059 - accuracy: 0.6795"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRAlzrXDtaSR"
      },
      "source": [
        "lstm_model.save('my_model.h5')\n",
        "test_y_pred = lstm_model.predict(test_x)\n",
        "y_pred = test_y_pred.argmax(1)\n",
        "prepare_output(test_IDs, y_pred)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb8vX4lNA80J",
        "outputId": "944f1353-8e19-41f8-ad8b-486252ee3ac9"
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "print('\\nTraining LSTM...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "model.add(LSTM(units=emdedding_size))\n",
        "model.add(Dense(units=10))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training LSTM...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvJfj13TtYw5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0almSP2v3mev"
      },
      "source": [
        "class Word2VecVectorizer:\n",
        "  def __init__(self, model):\n",
        "    print(\"Loading in word vectors...\")\n",
        "    self.word_vectors = model\n",
        "    print(\"Finished loading in word vectors\")\n",
        "\n",
        "  def fit(self, data):\n",
        "    pass\n",
        "\n",
        "  def transform(self, data):\n",
        "    v = self.word_vectors.get_vector('در')\n",
        "    self.D = v.shape[0]\n",
        "\n",
        "    X = np.zeros((len(data), self.D))\n",
        "    n = 0\n",
        "    emptycount = 0\n",
        "    for sentence in data:\n",
        "      tokens = sentence.split()\n",
        "      vecs = []\n",
        "      m = 0\n",
        "      for word in tokens:\n",
        "        try:\n",
        "          vec = self.word_vectors.get_vector(word)\n",
        "          vecs.append(vec)\n",
        "          m += 1\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if len(vecs) > 0:\n",
        "        vecs = np.array(vecs)\n",
        "        X[n] = vecs.mean(axis=0)\n",
        "      else:\n",
        "        emptycount += 1\n",
        "      n += 1\n",
        "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
        "    return X\n",
        "\n",
        "\n",
        "  def fit_transform(self, data):\n",
        "    self.fit(data)\n",
        "    return self.transform(data)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG4pUkdM4HKR",
        "outputId": "774b434c-3062-4718-a793-14ca839ad751"
      },
      "source": [
        "vectorizer = Word2VecVectorizer(model)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading in word vectors...\n",
            "Finished loading in word vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e4GAUco7pAw",
        "outputId": "f7684a25-c869-4df0-81f4-7a26b8afb1f2"
      },
      "source": [
        "Xtrain = vectorizer.fit_transform(train_X)\n",
        "Ytrain = train_Y"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numer of samples with no words found: 2 / 37899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8mPeIeU72M6",
        "outputId": "524be674-8c2b-4230-ad48-58d1550326ca"
      },
      "source": [
        "Xtest = vectorizer.transform(test_X)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numer of samples with no words found: 0 / 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZb6-Eqp8Dr0",
        "outputId": "d02dc7f8-9d88-4654-d714-471e4237c887"
      },
      "source": [
        "Xtrain[0].shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPRYc5U78LEC",
        "outputId": "49ec0dec-45e1-415b-d06b-ac41607317df"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=200)\n",
        "\n",
        "clf.fit(Xtrain, Ytrain)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzZLMvN08WVp"
      },
      "source": [
        "y_pred = clf.predict(Xtest)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMj5soyu86jK"
      },
      "source": [
        "prepare_output(test_IDs, y_pred)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-LEOC70B1Er"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "parameters = { \n",
        "    'C': [1.0, 10],\n",
        "    'gamma': [1, 'auto', 'scale']\n",
        "}\n",
        "\n",
        "model = GridSearchCV(SVC(kernel='rbf'), parameters, cv=5, n_jobs=-1).fit(Xtrain, Ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YTnMVDQC7od",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2ce325-9ce3-4a2b-c2ca-45f30b7932ef"
      },
      "source": [
        "embedding_vector_features=50\n",
        "\n",
        "model=tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(3877, embedding_vector_features,input_length=150))\n",
        "model.add(tf.keras.layers.LSTM(128,input_shape=([150, 50]),activation='relu',return_sequences=True))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.LSTM(128,activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 150, 50)           193850    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 150, 128)          91648     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 150, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 421,540\n",
            "Trainable params: 421,540\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-lo5zTS1BVn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}