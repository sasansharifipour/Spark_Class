{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark-First.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgF0wqH1yzVR9fwEKJnqwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasansharifipour/Spark_Class/blob/main/Spark_First.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuAusLVLQkZE"
      },
      "source": [
        "For installing **Spark** on google colab  we should follow several steps : <br>\r\n",
        "1. Install Java version 8  <br>\r\n",
        "2. Download **Spark**  <br>\r\n",
        "3. Extract compressed file <br>\r\n",
        "4. Install **finspark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipHMkLiHSTCs"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\r\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWHbDwkNQjOT"
      },
      "source": [
        "Now we should add JAVA_HOME and SPARK_HOME to os environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xheBMG2eS0S7"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP2gHsdpWVyM"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js7ehghyTCWT"
      },
      "source": [
        "The entry point for every **Spark** program is SparkContext.\r\n",
        "\r\n",
        "It can be fetch in spark-shell by **sc** or can be created by command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "WX-b2U0ZTjMr",
        "outputId": "72088f05-a591-41aa-b65a-99cc14c606d5"
      },
      "source": [
        "from pyspark import SparkContext\r\n",
        "spark_context = SparkContext()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7e7bcb48338d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pySpark Sql basic example, master=local[*]) created by getOrCreate at <ipython-input-21-978af12a1e1d>:3 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUH3vUMzK9iM"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "spark = SparkSession.builder.appName(\"pySpark Sql basic example\")\\\r\n",
        "        .getOrCreate()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEWCmh_2PjaM",
        "outputId": "1079465a-2eb2-48a4-c321-42e0c5452da5"
      },
      "source": [
        "rdd = spark.sparkContext.parallelize(range(10))\r\n",
        "rdd"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[1] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9BcM7TKPudj",
        "outputId": "620b77c8-c46a-443a-cb78-e2f9ce01bf5b"
      },
      "source": [
        "rdd.collect()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Rs2DTWPyr7"
      },
      "source": [
        "spark_context = spark._sc"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd9l4YE3QLIq"
      },
      "source": [
        "spark_context.stop()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BrIQHiNLV3y"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeeZ_yKCXXrK"
      },
      "source": [
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbeNFwTJYLzb",
        "outputId": "2fcfed9a-d7d4-419b-f911-1f273f768b01"
      },
      "source": [
        "rdd1 = sc.parallelize([1, 2, 3, 4])\r\n",
        "rdd1.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKsawdkvM9du",
        "outputId": "1a7f46b6-2355-4635-fd25-d9cf161335bb"
      },
      "source": [
        "rdd2 = sc.parallelize(range(10))\r\n",
        "rdd2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[3] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx68xMl8NGvD",
        "outputId": "f87652ec-5cf8-4b29-835a-c290195f591d"
      },
      "source": [
        "rdd2.collect()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyg-vuzNNKQ7",
        "outputId": "88d3b49c-c0db-4649-c98c-b2fa19db3656"
      },
      "source": [
        "rdd2.map(lambda x : x*x).collect()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSNGpwTPWcWb"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}